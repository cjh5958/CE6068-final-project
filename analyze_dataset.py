#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Dataset Analysis Tool for Leukemia Gene Expression Data
æ”¯æ´å°å·²ç”Ÿæˆçš„è³‡æ–™é›†é€²è¡Œå…¨é¢çš„å“è³ªåˆ†æå’Œè¦–è¦ºåŒ–æª¢æ¸¬
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from scipy.stats import pearsonr, spearmanr, ks_2samp
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å˜—è©¦å°å…¥ UMAPï¼ˆå¦‚æœæ²’å®‰è£æœƒçµ¦å‡ºå‹å–„æç¤ºï¼‰
try:
    import umap.umap_ as umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("âš ï¸  UMAP æœªå®‰è£ï¼Œå°‡è·³é UMAP åˆ†æã€‚å®‰è£æŒ‡ä»¤: pip install umap-learn")

class DatasetAnalyzer:
    """
    å…¨é¢çš„è³‡æ–™é›†åˆ†æå™¨
    æ”¯æ´ PCAã€t-SNEã€UMAPã€ç›¸é—œæ€§åˆ†æç­‰å¤šç¶­åº¦æª¢æ¸¬
    """
    
    def __init__(self, output_base_dir: str = "datasets/plots"):
        self.output_base_dir = output_base_dir
        self.analysis_results = {}
        
        # è¨­å®šå­—å‹ (ç§»é™¤ä¸­æ–‡å­—å‹è¨­å®šä»¥é¿å…å­—é«”å•é¡Œ)
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ†æåƒæ•¸
        self.pca_components = 2
        self.tsne_params = {'n_components': 2, 'random_state': 42, 'perplexity': 30}
        self.umap_params = {'n_components': 2, 'random_state': 42, 'n_neighbors': 15}
        
        # é¡è‰²é…ç½®
        self.colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', 
                      '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43']
    
    def load_dataset(self, dataset_path: str) -> tuple:
        """
        è¼‰å…¥è³‡æ–™é›†ï¼ˆæ”¯æ´ CSV å’Œ PKL æ ¼å¼ï¼‰
        
        Returns:
            features: åŸºå› è¡¨é”ç‰¹å¾µçŸ©é™£
            labels: é¡åˆ¥æ¨™ç±¤
            label_names: é¡åˆ¥åç¨±
            dataset_name: è³‡æ–™é›†åç¨±
        """
        print(f"ğŸ“ è¼‰å…¥è³‡æ–™é›†: {dataset_path}")
        
        # æå–è³‡æ–™é›†åç¨±
        dataset_name = os.path.splitext(os.path.basename(dataset_path))[0]
        
        if dataset_path.endswith('.csv'):
            df = pd.read_csv(dataset_path)
            
            # åˆ†é›¢ç‰¹å¾µå’Œæ¨™ç±¤
            if 'type' in df.columns:
                features = df.drop(['type', 'samples'], axis=1, errors='ignore').values
                labels = df['type'].values
                unique_labels = np.unique(labels)
                
                # å°‡é¡åˆ¥åç¨±è½‰æ›ç‚ºæ•¸å€¼
                label_mapping = {label: idx for idx, label in enumerate(unique_labels)}
                numeric_labels = np.array([label_mapping[label] for label in labels])
                
                return features, numeric_labels, unique_labels, dataset_name
            else:
                raise ValueError("CSV æª”æ¡ˆå¿…é ˆåŒ…å« 'type' æ¬„ä½")
                
        elif dataset_path.endswith('.pkl'):
            with open(dataset_path, 'rb') as f:
                data = pickle.load(f)
            
            features = data['features']
            labels = data['labels']
            
            if 'class_names' in data:
                label_names = data['class_names']
            else:
                label_names = [f'Class_{i}' for i in range(len(np.unique(labels)))]
            
            return features, labels, label_names, dataset_name
        else:
            raise ValueError("åƒ…æ”¯æ´ .csv å’Œ .pkl æ ¼å¼")
    
    def create_output_directory(self, dataset_name: str) -> str:
        """
        å‰µå»ºä»¥è³‡æ–™é›†åç¨±å‘½åçš„è¼¸å‡ºç›®éŒ„
        """
        output_dir = os.path.join(self.output_base_dir, f"{dataset_name}_analysis")
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“‚ åˆ†æçµæœå°‡ä¿å­˜è‡³: {output_dir}")
        return output_dir
    
    def perform_pca_analysis(self, features: np.ndarray, labels: np.ndarray, 
                           label_names: list, output_dir: str):
        """
        é€²è¡Œ PCA åˆ†æä¸¦ç”Ÿæˆè¦–è¦ºåŒ–
        """
        print("ğŸ” åŸ·è¡Œ PCA åˆ†æ...")
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # PCA é™ç¶­
        pca = PCA(n_components=min(50, features.shape[1]))  # æœ€å¤šè¨ˆç®—50å€‹ä¸»æˆåˆ†
        pca_features = pca.fit_transform(features_scaled)
        
        # å„²å­˜ PCA çµæœ
        self.analysis_results['pca'] = {
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'cumulative_variance': np.cumsum(pca.explained_variance_ratio_),
            'components': pca.components_,
            'features_2d': pca_features[:, :2]
        }
        
        # å‰µå»º PCA åˆ†æåœ–è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('PCA Principal Component Analysis Results', fontsize=16, fontweight='bold')
        
        # 1. 2D PCA æ•£é»åœ–
        ax1 = axes[0, 0]
        for i, label_name in enumerate(label_names):
            mask = labels == i
            if np.any(mask):
                ax1.scatter(pca_features[mask, 0], pca_features[mask, 1], 
                           c=self.colors[i % len(self.colors)], label=label_name, 
                           alpha=0.7, s=50)
        
        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} Variance)')
        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} Variance)')
        ax1.set_title('PCA 2D Projection')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. è§£é‡‹è®Šç•°æ¯”ä¾‹
        ax2 = axes[0, 1]
        n_components = min(20, len(pca.explained_variance_ratio_))
        ax2.bar(range(1, n_components+1), pca.explained_variance_ratio_[:n_components])
        ax2.set_xlabel('Principal Component Number')
        ax2.set_ylabel('Explained Variance Ratio')
        ax2.set_title('Individual Component Variance')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç´¯ç©è§£é‡‹è®Šç•°
        ax3 = axes[1, 0]
        ax3.plot(range(1, n_components+1), 
                np.cumsum(pca.explained_variance_ratio_[:n_components]), 
                'bo-', linewidth=2, markersize=6)
        ax3.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% Variance')
        ax3.set_xlabel('Number of Components')
        ax3.set_ylabel('Cumulative Explained Variance')
        ax3.set_title('Cumulative Variance Explained')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. é¡åˆ¥åˆ†é›¢åº¦è©•ä¼°
        ax4 = axes[1, 1]
        if len(label_names) > 1:
            silhouette_avg = silhouette_score(pca_features[:, :2], labels)
            ax4.text(0.5, 0.7, f'Silhouette Score: {silhouette_avg:.3f}', 
                    ha='center', va='center', fontsize=14, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            ax4.text(0.5, 0.5, f'First 2 PCs Variance: {np.sum(pca.explained_variance_ratio_[:2]):.2%}', 
                    ha='center', va='center', fontsize=12)
            ax4.text(0.5, 0.3, f'Sample Count: {features.shape[0]}', 
                    ha='center', va='center', fontsize=12)
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.set_title('PCA Quality Assessment')
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'pca_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… PCA åˆ†æå®Œæˆ - å‰2å€‹ä¸»æˆåˆ†è§£é‡‹ {np.sum(pca.explained_variance_ratio_[:2]):.2%} çš„è®Šç•°")
    
    def perform_tsne_analysis(self, features: np.ndarray, labels: np.ndarray, 
                            label_names: list, output_dir: str):
        """
        é€²è¡Œ t-SNE åˆ†æä¸¦ç”Ÿæˆè¦–è¦ºåŒ–
        """
        print("ğŸ” åŸ·è¡Œ t-SNE åˆ†æ...")
        
        # é™åˆ¶æ¨£æœ¬æ•¸é‡ä»¥é¿å… t-SNE è¨ˆç®—éæ…¢
        if features.shape[0] > 1000:
            print(f"âš ï¸  æ¨£æœ¬æ•¸éå¤š ({features.shape[0]})ï¼Œéš¨æ©Ÿæ¡æ¨£1000å€‹æ¨£æœ¬é€²è¡Œ t-SNE åˆ†æ")
            indices = np.random.choice(features.shape[0], 1000, replace=False)
            features_sample = features[indices]
            labels_sample = labels[indices]
        else:
            features_sample = features
            labels_sample = labels
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_sample)
        
        # t-SNE é™ç¶­
        tsne = TSNE(**self.tsne_params)
        tsne_features = tsne.fit_transform(features_scaled)
        
        # å„²å­˜ t-SNE çµæœ
        self.analysis_results['tsne'] = {
            'features_2d': tsne_features,
            'labels': labels_sample
        }
        
        # å‰µå»º t-SNE åˆ†æåœ–è¡¨
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('t-SNE Nonlinear Dimensionality Reduction Results', fontsize=16, fontweight='bold')
        
        # 1. t-SNE æ•£é»åœ–
        ax1 = axes[0]
        for i, label_name in enumerate(label_names):
            mask = labels_sample == i
            if np.any(mask):
                ax1.scatter(tsne_features[mask, 0], tsne_features[mask, 1], 
                           c=self.colors[i % len(self.colors)], label=label_name, 
                           alpha=0.7, s=50)
        
        ax1.set_xlabel('t-SNE 1')
        ax1.set_ylabel('t-SNE 2')
        ax1.set_title('t-SNE 2D Projection')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å“è³ªè©•ä¼°
        ax2 = axes[1]
        if len(label_names) > 1:
            silhouette_avg = silhouette_score(tsne_features, labels_sample)
            ax2.text(0.5, 0.7, f'Silhouette Score: {silhouette_avg:.3f}', 
                    ha='center', va='center', fontsize=14, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
            ax2.text(0.5, 0.5, f'Perplexity: {self.tsne_params["perplexity"]}', 
                    ha='center', va='center', fontsize=12)
            ax2.text(0.5, 0.3, f'Sample Count: {features_sample.shape[0]}', 
                    ha='center', va='center', fontsize=12)
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.set_title('t-SNE Quality Assessment')
        ax2.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'tsne_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… t-SNE åˆ†æå®Œæˆ")
    
    def perform_umap_analysis(self, features: np.ndarray, labels: np.ndarray, 
                            label_names: list, output_dir: str):
        """
        é€²è¡Œ UMAP åˆ†æä¸¦ç”Ÿæˆè¦–è¦ºåŒ–
        """
        if not UMAP_AVAILABLE:
            print("â­ï¸  è·³é UMAP åˆ†æï¼ˆæœªå®‰è£ umap-learnï¼‰")
            return
        
        print("ğŸ” åŸ·è¡Œ UMAP åˆ†æ...")
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # UMAP é™ç¶­
        reducer = umap.UMAP(**self.umap_params)
        umap_features = reducer.fit_transform(features_scaled)
        
        # å„²å­˜ UMAP çµæœ
        self.analysis_results['umap'] = {
            'features_2d': umap_features,
            'labels': labels
        }
        
        # å‰µå»º UMAP åˆ†æåœ–è¡¨
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('UMAP Uniform Manifold Approximation Results', fontsize=16, fontweight='bold')
        
        # 1. UMAP æ•£é»åœ–
        ax1 = axes[0]
        for i, label_name in enumerate(label_names):
            mask = labels == i
            if np.any(mask):
                ax1.scatter(umap_features[mask, 0], umap_features[mask, 1], 
                           c=self.colors[i % len(self.colors)], label=label_name, 
                           alpha=0.7, s=50)
        
        ax1.set_xlabel('UMAP 1')
        ax1.set_ylabel('UMAP 2')
        ax1.set_title('UMAP 2D Projection')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å“è³ªè©•ä¼°
        ax2 = axes[1]
        if len(label_names) > 1:
            silhouette_avg = silhouette_score(umap_features, labels)
            ax2.text(0.5, 0.7, f'Silhouette Score: {silhouette_avg:.3f}', 
                    ha='center', va='center', fontsize=14, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
            ax2.text(0.5, 0.5, f'Neighbors: {self.umap_params["n_neighbors"]}', 
                    ha='center', va='center', fontsize=12)
            ax2.text(0.5, 0.3, f'Sample Count: {features.shape[0]}', 
                    ha='center', va='center', fontsize=12)
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.set_title('UMAP Quality Assessment')
        ax2.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'umap_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… UMAP åˆ†æå®Œæˆ")
    
    def perform_correlation_analysis(self, features: np.ndarray, output_dir: str, 
                                   max_features: int = 100):
        """
        é€²è¡Œç›¸é—œæ€§åˆ†æä¸¦ç”Ÿæˆç†±åŠ›åœ–
        """
        print("ğŸ” åŸ·è¡Œç›¸é—œæ€§åˆ†æ...")
        
        # å¦‚æœç‰¹å¾µå¤ªå¤šï¼Œé¸æ“‡æ–¹å·®æœ€å¤§çš„ç‰¹å¾µé€²è¡Œåˆ†æ
        if features.shape[1] > max_features:
            feature_vars = np.var(features, axis=0)
            top_indices = np.argsort(feature_vars)[-max_features:]
            features_selected = features[:, top_indices]
            print(f"ğŸ“Š é¸æ“‡æ–¹å·®æœ€å¤§çš„ {max_features} å€‹ç‰¹å¾µé€²è¡Œç›¸é—œæ€§åˆ†æ")
        else:
            features_selected = features
            top_indices = np.arange(features.shape[1])
        
        # è¨ˆç®—çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸
        correlation_matrix = np.corrcoef(features_selected.T)
        
        # å„²å­˜ç›¸é—œæ€§çµæœ
        self.analysis_results['correlation'] = {
            'correlation_matrix': correlation_matrix,
            'selected_features': top_indices
        }
        
        # å‰µå»ºç›¸é—œæ€§åˆ†æåœ–è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(16, 14))
        fig.suptitle('Feature Correlation Analysis Results', fontsize=16, fontweight='bold')
        
        # 1. ç›¸é—œæ€§ç†±åŠ›åœ–
        ax1 = axes[0, 0]
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', 
                   center=0, square=True, ax=ax1, cbar_kws={"shrink": .8})
        ax1.set_title('Feature Correlation Heatmap')
        
        # 2. ç›¸é—œæ€§åˆ†å¸ƒç›´æ–¹åœ–
        ax2 = axes[0, 1]
        corr_values = correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]
        ax2.hist(corr_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(np.mean(corr_values), color='red', linestyle='--', 
                   label=f'Mean: {np.mean(corr_values):.3f}')
        ax2.set_xlabel('Correlation Coefficient')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Correlation Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. é«˜ç›¸é—œæ€§ç‰¹å¾µå°
        ax3 = axes[1, 0]
        high_corr_threshold = 0.8
        high_corr_pairs = np.where((np.abs(correlation_matrix) > high_corr_threshold) & 
                                  (correlation_matrix != 1.0))
        
        if len(high_corr_pairs[0]) > 0:
            high_corr_values = correlation_matrix[high_corr_pairs]
            ax3.scatter(range(len(high_corr_values)), high_corr_values, 
                       c='red', alpha=0.7, s=50)
            ax3.axhline(y=high_corr_threshold, color='orange', linestyle='--', 
                       label=f'Threshold: {high_corr_threshold}')
            ax3.axhline(y=-high_corr_threshold, color='orange', linestyle='--')
            ax3.set_xlabel('Feature Pair Index')
            ax3.set_ylabel('Correlation Coefficient')
            ax3.set_title(f'High Correlation Pairs (|r| > {high_corr_threshold})')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
            ax3.text(0.02, 0.98, f'High Corr Pairs: {len(high_corr_values)}', 
                    transform=ax3.transAxes, va='top', fontsize=10,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        else:
            ax3.text(0.5, 0.5, 'No High Correlation Pairs', ha='center', va='center', 
                    fontsize=14, transform=ax3.transAxes)
            ax3.set_title(f'High Correlation Pairs (|r| > {high_corr_threshold})')
        
        # 4. çµ±è¨ˆæ‘˜è¦
        ax4 = axes[1, 1]
        stats_text = f"""
Correlation Statistics Summary:

â€¢ Features Analyzed: {features_selected.shape[1]}
â€¢ Mean Correlation: {np.mean(corr_values):.4f}
â€¢ Std Correlation: {np.std(corr_values):.4f}
â€¢ Max Correlation: {np.max(corr_values):.4f}
â€¢ Min Correlation: {np.min(corr_values):.4f}
â€¢ High Pairs (|r|>0.8): {len(high_corr_pairs[0])//2}
â€¢ Medium Pairs (0.5<|r|<0.8): {len(np.where((np.abs(corr_values) > 0.5) & (np.abs(corr_values) <= 0.8))[0])}
        """
        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, va='top', 
                fontsize=11, fontfamily='monospace')
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'correlation_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ç›¸é—œæ€§åˆ†æå®Œæˆ")
    
    def perform_statistical_analysis(self, features: np.ndarray, labels: np.ndarray, 
                                   label_names: list, output_dir: str):
        """
        é€²è¡Œçµ±è¨ˆç‰¹æ€§åˆ†æ
        """
        print("ğŸ” åŸ·è¡Œçµ±è¨ˆç‰¹æ€§åˆ†æ...")
        
        # å‰µå»ºçµ±è¨ˆåˆ†æåœ–è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Dataset Statistical Analysis', fontsize=16, fontweight='bold')
        
        # 1. é¡åˆ¥åˆ†å¸ƒ
        ax1 = axes[0, 0]
        unique_labels, counts = np.unique(labels, return_counts=True)
        colors_subset = [self.colors[i % len(self.colors)] for i in range(len(unique_labels))]
        
        bars = ax1.bar([label_names[i] for i in unique_labels], counts, color=colors_subset)
        ax1.set_xlabel('Leukemia Type')
        ax1.set_ylabel('Sample Count')
        ax1.set_title('Class Distribution')
        ax1.tick_params(axis='x', rotation=45)
        
        # åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸å€¼
        for bar, count in zip(bars, counts):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    str(count), ha='center', va='bottom')
        
        # 2. ç‰¹å¾µåˆ†å¸ƒç›´æ–¹åœ–
        ax2 = axes[0, 1]
        feature_means = np.mean(features, axis=0)
        ax2.hist(feature_means, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
        ax2.set_xlabel('Feature Mean Value')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Feature Mean Distribution')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç‰¹å¾µè®Šç•°æ€§
        ax3 = axes[0, 2]
        feature_stds = np.std(features, axis=0)
        ax3.hist(feature_stds, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.set_xlabel('Feature Standard Deviation')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Feature Variability Distribution')
        ax3.grid(True, alpha=0.3)
        
        # 4. æ¨£æœ¬é–“è·é›¢åˆ†å¸ƒ
        ax4 = axes[1, 0]
        # éš¨æ©Ÿæ¡æ¨£è¨ˆç®—è·é›¢ï¼ˆé¿å…è¨ˆç®—é‡éå¤§ï¼‰
        if features.shape[0] > 100:
            sample_indices = np.random.choice(features.shape[0], 100, replace=False)
            features_sample = features[sample_indices]
        else:
            features_sample = features
        
        from sklearn.metrics import pairwise_distances
        distances = pairwise_distances(features_sample, metric='euclidean')
        distance_values = distances[np.triu_indices_from(distances, k=1)]
        
        ax4.hist(distance_values, bins=50, alpha=0.7, color='gold', edgecolor='black')
        ax4.set_xlabel('Euclidean Distance')
        ax4.set_ylabel('Frequency')
        ax4.set_title('Inter-sample Distance Distribution')
        ax4.grid(True, alpha=0.3)
        
        # 5. å„é¡åˆ¥ç‰¹å¾µå‡å€¼æ¯”è¼ƒï¼ˆç®±ç·šåœ–ï¼‰
        ax5 = axes[1, 1]
        if len(label_names) <= 10:  # é¿å…åœ–è¡¨éæ–¼æ“æ“ 
            # é¸æ“‡æ–¹å·®æœ€å¤§çš„å‰10å€‹ç‰¹å¾µ
            feature_vars = np.var(features, axis=0)
            top_feature_indices = np.argsort(feature_vars)[-10:]
            
            data_for_box = []
            labels_for_box = []
            
            for class_idx in unique_labels:
                class_mask = labels == class_idx
                class_features = features[class_mask][:, top_feature_indices]
                data_for_box.extend(class_features.flatten())
                labels_for_box.extend([label_names[class_idx]] * class_features.size)
            
            # å‰µå»ºDataFrameä»¥ä¾¿ä½¿ç”¨seaborn
            import pandas as pd
            df_box = pd.DataFrame({
                'value': data_for_box,
                'class': labels_for_box
            })
            
            sns.boxplot(data=df_box, x='class', y='value', ax=ax5)
            ax5.set_xlabel('Leukemia Type')
            ax5.set_ylabel('Feature Value')
            ax5.set_title('Feature Value Distribution by Class')
            ax5.tick_params(axis='x', rotation=45)
        else:
            ax5.text(0.5, 0.5, 'Too Many Classes for Box Plot', ha='center', va='center', 
                    transform=ax5.transAxes, fontsize=12)
        
        # 6. è³‡æ–™å“è³ªè©•ä¼°
        ax6 = axes[1, 2]
        
        # è¨ˆç®—å“è³ªæŒ‡æ¨™
        n_samples, n_features = features.shape
        feature_completeness = 1.0  # å‡è¨­æ²’æœ‰ç¼ºå¤±å€¼
        class_balance = 1 - np.std(counts) / np.mean(counts) if len(counts) > 1 else 1.0
        
        quality_metrics = [
            ('Sample Count', n_samples),
            ('Feature Count', n_features),
            ('Class Count', len(unique_labels)),
            ('Class Balance', f'{class_balance:.3f}'),
            ('Data Completeness', f'{feature_completeness:.3f}'),
            ('Mean Feature Value', f'{np.mean(features):.3f}'),
            ('Feature Std Dev', f'{np.std(features):.3f}')
        ]
        
        y_pos = 0.9
        for metric, value in quality_metrics:
            ax6.text(0.05, y_pos, f'{metric}: {value}', transform=ax6.transAxes, 
                    fontsize=11, fontfamily='monospace')
            y_pos -= 0.12
        
        ax6.set_xlim(0, 1)
        ax6.set_ylim(0, 1)
        ax6.set_title('Data Quality Metrics')
        ax6.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'statistical_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… çµ±è¨ˆç‰¹æ€§åˆ†æå®Œæˆ")
    
    def compare_dimensionality_reduction(self, output_dir: str):
        """
        æ¯”è¼ƒä¸åŒé™ç¶­æ–¹æ³•çš„æ•ˆæœ
        """
        if 'pca' not in self.analysis_results:
            print("âš ï¸  ç„¡æ³•é€²è¡Œé™ç¶­æ–¹æ³•æ¯”è¼ƒï¼Œç¼ºå°‘ PCA çµæœ")
            return
        
        print("ğŸ” æ¯”è¼ƒä¸åŒé™ç¶­æ–¹æ³•...")
        
        available_methods = []
        if 'pca' in self.analysis_results:
            available_methods.append('PCA')
        if 'tsne' in self.analysis_results:
            available_methods.append('t-SNE')
        if 'umap' in self.analysis_results:
            available_methods.append('UMAP')
        
        if len(available_methods) <= 1:
            print("âš ï¸  å¯ç”¨é™ç¶­æ–¹æ³•å°‘æ–¼2ç¨®ï¼Œè·³éæ¯”è¼ƒåˆ†æ")
            return
        
        # å‰µå»ºæ¯”è¼ƒåœ–è¡¨
        fig, axes = plt.subplots(1, len(available_methods), figsize=(6*len(available_methods), 6))
        if len(available_methods) == 1:
            axes = [axes]
        
        fig.suptitle('Dimensionality Reduction Methods Comparison', fontsize=16, fontweight='bold')
        
        # ç²å–é¡åˆ¥åç¨±
        label_names = self.analysis_results.get('label_names', [])
        
        for idx, method in enumerate(available_methods):
            ax = axes[idx]
            
            if method == 'PCA':
                features_2d = self.analysis_results['pca']['features_2d']
                labels = self.analysis_results.get('labels', np.array([]))
            elif method == 't-SNE':
                features_2d = self.analysis_results['tsne']['features_2d']
                labels = self.analysis_results['tsne']['labels']
            elif method == 'UMAP':
                features_2d = self.analysis_results['umap']['features_2d']
                labels = self.analysis_results['umap']['labels']
            
            # ç¹ªè£½æ•£é»åœ–
            unique_labels = np.unique(labels)
            for i, label_idx in enumerate(unique_labels):
                mask = labels == label_idx
                if np.any(mask):
                    # ä½¿ç”¨å¯¦éš›é¡åˆ¥åç¨±è€Œä¸æ˜¯ Class ç·¨è™Ÿ
                    label_name = label_names[label_idx] if label_idx < len(label_names) else f'Class_{label_idx}'
                    ax.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                             c=self.colors[i % len(self.colors)], 
                             alpha=0.7, s=30, label=label_name)
            
            ax.set_title(f'{method} Results')
            ax.set_xlabel(f'{method} 1')
            ax.set_ylabel(f'{method} 2')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'dimensionality_reduction_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… é™ç¶­æ–¹æ³•æ¯”è¼ƒå®Œæˆ")
    
    def analyze_dataset(self, dataset_path: str, 
                       include_pca: bool = True,
                       include_tsne: bool = True, 
                       include_umap: bool = True,
                       include_correlation: bool = True,
                       include_statistics: bool = True,
                       max_correlation_features: int = 100):
        """
        å°è³‡æ–™é›†é€²è¡Œå…¨é¢åˆ†æ
        """
        print("ğŸš€ é–‹å§‹è³‡æ–™é›†å…¨é¢åˆ†æ...")
        print("=" * 60)
        
        # è¼‰å…¥è³‡æ–™é›†
        features, labels, label_names, dataset_name = self.load_dataset(dataset_path)
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_dir = self.create_output_directory(dataset_name)
        
        # å„²å­˜æ¨™ç±¤ä¿¡æ¯ä¾›å¾ŒçºŒä½¿ç”¨
        self.analysis_results['labels'] = labels
        self.analysis_results['label_names'] = label_names
        
        print(f"\nğŸ“Š è³‡æ–™é›†è³‡è¨Š:")
        print(f"   æ¨£æœ¬æ•¸: {features.shape[0]}")
        print(f"   ç‰¹å¾µæ•¸: {features.shape[1]}")
        print(f"   é¡åˆ¥æ•¸: {len(label_names)}")
        print(f"   é¡åˆ¥åç¨±: {label_names}")
        print("=" * 60)
        
        # åŸ·è¡Œå„é …åˆ†æ
        if include_pca:
            self.perform_pca_analysis(features, labels, label_names, output_dir)
        
        if include_tsne:
            self.perform_tsne_analysis(features, labels, label_names, output_dir)
        
        if include_umap:
            self.perform_umap_analysis(features, labels, label_names, output_dir)
        
        if include_correlation:
            self.perform_correlation_analysis(features, output_dir, max_correlation_features)
        
        if include_statistics:
            self.perform_statistical_analysis(features, labels, label_names, output_dir)
        
        # æ¯”è¼ƒé™ç¶­æ–¹æ³•
        self.compare_dimensionality_reduction(output_dir)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è³‡æ–™é›†åˆ†æå®Œæˆï¼")
        print(f"ğŸ“‚ çµæœä¿å­˜åœ¨: {output_dir}")
        print("=" * 60)
        
        return output_dir

def main():
    parser = argparse.ArgumentParser(description='å…¨é¢çš„è³‡æ–™é›†åˆ†æå·¥å…·')
    parser.add_argument('dataset_path', help='è³‡æ–™é›†è·¯å¾‘ (.csv æˆ– .pkl)')
    parser.add_argument('--output-dir', default='datasets/plots', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--skip-pca', action='store_true', help='è·³é PCA åˆ†æ')
    parser.add_argument('--skip-tsne', action='store_true', help='è·³é t-SNE åˆ†æ')
    parser.add_argument('--skip-umap', action='store_true', help='è·³é UMAP åˆ†æ')
    parser.add_argument('--skip-correlation', action='store_true', help='è·³éç›¸é—œæ€§åˆ†æ')
    parser.add_argument('--skip-statistics', action='store_true', help='è·³éçµ±è¨ˆåˆ†æ')
    parser.add_argument('--max-corr-features', type=int, default=100, 
                       help='ç›¸é—œæ€§åˆ†ææœ€å¤§ç‰¹å¾µæ•¸')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.dataset_path):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è³‡æ–™é›†æª”æ¡ˆ {args.dataset_path}")
        return
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DatasetAnalyzer(args.output_dir)
    
    # åŸ·è¡Œåˆ†æ
    try:
        output_dir = analyzer.analyze_dataset(
            dataset_path=args.dataset_path,
            include_pca=not args.skip_pca,
            include_tsne=not args.skip_tsne,
            include_umap=not args.skip_umap,
            include_correlation=not args.skip_correlation,
            include_statistics=not args.skip_statistics,
            max_correlation_features=args.max_corr_features
        )
        
        print(f"\nâœ¨ åˆ†æå®Œæˆï¼è«‹æŸ¥çœ‹çµæœç›®éŒ„: {output_dir}")
        
    except Exception as e:
        print(f"âŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 